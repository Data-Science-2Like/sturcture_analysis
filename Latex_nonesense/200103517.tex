
\documentclass[journal=jcisd8,manuscript=article]{achemso}

\usepackage[final]{changes}
\usepackage{amsmath,graphicx}
\usepackage{amssymb}
\usepackage[T1]{fontenc}  \usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{standalone}



\usepackage{hyperref}


\usepackage{psfrag}



\title{Autoencoding Undirected Molecular Graphs With Neural Networks}

\author{Jeppe Johan Waarkjær Olsen}
\affiliation{Department of Computing, Technical University of Denmark}

\author{Peter Ebert Christensen}
\affiliation{Department of Computing, Technical University of Denmark}



\author{Martin Hangaard Hansen}

\author{Alexander Rosenberg Johansen}
\affiliation{Department of Computing, Technical University of Denmark}
\email{aler@dtu.dk}


\begin{document}
\maketitle
\subfile{sections/abstract.tex}
\subfile{sections/introduction.tex}
\subfile{sections/methods.tex}
\subfile{sections/m_lmtask.tex}
\subfile{sections/m_counting.tex}
\subfile{sections/m_bagofvectors.tex}
\subfile{sections/m_transformer.tex}
\subfile{sections/e_setup.tex}
\subfile{sections/e_preprocessing.tex}
\subfile{sections/e_training.tex}
\subfile{sections/e_evaluation.tex}
\subfile{sections/results.tex}
\subfile{sections/r_qm9.tex}
\subfile{sections/r_zinc.tex}
\subfile{sections/qualitative_results.tex}
\subfile{sections/conclusion.tex}
\section{Acknowledgments}
This research is funded by the Innovation Foundation Denmark through the DABAI project


\bibliography{refs}
\newpage

\appendix
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thefigure}{S.\arabic{figure}}
\setcounter{figure}{0}
\setcounter{table}{0}
\section{Appendix}
\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,graphicx}
\usepackage{amssymb}
\usepackage[T1]{fontenc}  \usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{subfiles}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\title{Supporting information for Autoencoding undirected molecular graphs with neural networks}

\author{Jeppe Johan Waarkjær Olsen \and Peter Ebert Christensen \and
Martin Hangaard Hansen \and Alexander Rosenberg Johansen}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thefigure}{S.\arabic{figure}}

\begin{document}

\maketitle


\begin{table}[H]
    \centering
    \begin{tabular}{c|l}
        Variable & Description\\ \hline
        $G$ & Graph, defined as a set of nodes and edges (V,E) \\
        $V$ & Set of nodes (atoms) in the graph\\
        $V_\texttt{subset}$ & set of masked atoms. $|V_\texttt{subset}|=n_{corrupt}$\\
        $n_{corrupt}$ & Number of atoms corrupted per molecule\\
        $E$ & Adjacency matrix ($E_{ij}\in \{0,1,2,3\}$) \\
        $\widetilde{G}$ & Corrupted graph, with $V_\texttt{subset}$ replaced with a \texttt{<MASK>} token. \\
        $v_i$ & I'th atom in the graph. $v_i=(a,i)$\\
        $a$ & Element of an atom. $a\in \{H,C,O,N,F,P,S,Cl,Br,I\}$\\
        $x$ & Token represented as a vector. $x\in\mathbb{R}^d$\\
        $\texttt{embedding}(x)$ & Embedding of a token. $\texttt{embedding}(x)\in \mathbb{R}^{d_{emb}}$\\
        $z_\theta$ & Intermediate representation in the BoW model. $z_\theta\in \mathbb{R}^{d_{emb}}$ \\
        $h_\theta$ & Hidden representation in the BoW model. $h_\theta \in \mathbb{R}^{d_{nn}}$\\
        $h^0$ & Embedding of the nodes in the graph. $h^0\in \mathbb{R}^{|V|\times d_{emb}}$\\
        $z^l, h^l$ & Intermediate and hidden representation of the l'th layer. $z^l, h^l \in \mathbb{R}^{|V|\times d_{transform}}$\\
        $e^V, e^K$ & Embeddings of the bonds. $e^V, e^K\in \mathbb{R}^{|V|\times|V|\times d_{transform}}$\\
        $W^Q,W^K,W^V$ & Trainable weights. $W^Q,W^K,W^V\in \mathbb{R}^{d_{transform}\times d_{transform}}$\\
        $\alpha_{ij}$ & Attention weights between i'th and j'th atom. $\alpha_{ij}\in [0,1], \sum_j \alpha_{ij}=1$\\
        $\phi_{ij}$ & Unnormalized attention weights. $\phi_{ij}\in \mathbb{R}$ \\
        $W_{multi}$ & Trainable weight. $W_{multi}\in \mathbb{R}^{(d_{transform}\cdot k)\times d_{transform}}$\\
        $\texttt{C\_i}$ & I'th head of attention function for an atom. $\texttt{C\_i}\in \mathbb{R}^{d_{transform}}$
        
        
        \end{tabular}
    \caption{Describtion of variables used.}
    \label{tab:my_label}
\end{table}


\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
        Model & Training time (min) & Dataset \\\hline
        \texttt{binary-transformer} & 110 & QM9 \\ \hline
        \texttt{binary-transformer} & 482 & ZINC \\ \hline
        \texttt{bond-transformer} & 112 & QM9 \\ \hline
        \texttt{bond-transformer} & 484 & ZINC \\ \hline
        \texttt{bag-of-atoms} & 71 & QM9 \\ \hline
        \texttt{bag-of-atoms} & 158 & ZINC \\ \hline
        \texttt{bag-of-neighbors} & 72 & QM9 \\ \hline
        \texttt{bag-of-neighbors} & 144 & ZINC \\ \hline

    \end{tabular}
    \caption{Training time of our different models, on the QM9 and ZINC datasets.}
    \label{tab:my_label}
\end{table}

\section{Epsilon-greedy}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/epsilon_greedy_qm9.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/epsilon_greedy_zinc.pdf}
    \end{subfigure}
    \caption{Validation perplexity of binary and bond transformer -- with and without $\epsilon$-greedy masking strategy -- with different number of masked atoms. (a) is on the QM9 dataset and (b) is on the ZINC dataset }
    \label{fig:epsilon_greedy}
\end{figure}

\section{K-smoothing}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/smoothing.pdf}
    \caption{Cross entropy as a function of k-smoothing evaluated on the ZINC validation dataset.}
    \label{fig:my_label}
\end{figure}

\section{Graph Attention}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/graph_attention1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/graph_attention2.pdf}
    \end{subfigure}
        \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/graph_attention3.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{SI/images/graph_attention4.pdf}
    \end{subfigure}
    \caption{Perplexity on validation dataset – with one atom masked per molecule – for each epoch of training. (a) is a bond transformer trained on QM9, (b) is a bond transformer trained on ZINC, (c) is a binary transformer trained on QM9 and (d) is a binary transformer trained on ZINC }
    \label{fig:attentions}
\end{figure}

 \section{QM9 extended results}
\label{sec:appendix:qm9}


From Table \ref{tab:n_mask} and Figure \ref{fig:f1_micro_vs_nmask_qm9},\ref{fig:f1_macro_vs_nmask_qm9} we see that as we mask more atoms per molecule, the \texttt{bond-transformer}, maintains a perfect score, since it can solve the task by only looking at the bonds. The \texttt{Binary-transformer} drops slightly in performance, as we mask more atoms. The \texttt{Bag-of-neighbors} doesn't seem to depend on the number of masked atoms. This indicates that the model most likely, base its predictions on the number of neighbors, which also can be an indication of the number of covalent bonds. As we remove information except compositional, the \texttt{bag-of-atoms} model drops significantly has we mask more atoms, reaching similar performance to the \texttt{Unigram}, as we approach fully masked molecules. This is no surprise, as a fully masked molecule, only gives the model information about the number of atoms, which should not be enough to infer anything.




\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|}
\hline
Model & Metric & $n_{mask}=1$
& $n_{mask}=5$ & all masked\\ \hline
\multirow{3}{*}{\texttt{octet-rule-unigram}}
 & acc & 100  & 100 & 100\\ 
 & f1  & 100  & 100 & 100\\
 & PP  & 1.002 & 1.002 & 1.002 \\ \hline
\multirow{3}{*}{\texttt{bond-transformer}}
 & acc & \textbf{99.99} $\pm$ 0.01 & 99.99 $\pm$ 0.01 & 100.0 $\pm$ 0.0  \\
 & f1  & \textbf{99.99}    $\pm$ 0.01 & 99.99 $\pm$ 0.01 & 100.0 $\pm$ 0.0   \\ & PP  & \textbf{1.002}  $\pm$ 0.001 & 1.002 $\pm$ 0.001  & 1.002 $\pm$ 0.001   \\ \hline
\multirow{3}{*}{\texttt{binary-transformer}}
 & acc & 99.73 $\pm$ 0.06 & 97.91 $\pm$ 0.08 & 95.75 $\pm$ 0.19\\
 & F1  & 99.73 $\pm$ 0.06 & 97.91 $\pm$ 0.08 &  95.75 $\pm$ 0.19\\
 & PP  & 1.009 $\pm$ 0.002 & 1.045 $\pm$ 0.002 & 1.094 $\pm$ 0.004\\ \hline
\multirow{3}{*}{\texttt{bag-of-neighbors}}
 & acc & 90.7 $\pm$ 0.1 & 90.2 $\pm$ 0.1 & 90.8 $\pm$ 0.1 \\ 
 & F1  & 90.7 $\pm$ 0.1 & 90.2 $\pm$ 0.1 & 90.8 $\pm$ 0.1\\
 & PP  & 1.281 $\pm$ 0.004 & 1.299 $\pm$ 0.003 & 1.319 $\pm$ 0.007  \\ \hline
\multirow{3}{*}{\texttt{bag-of-atoms}}
 & acc & 65.8 $\pm$ 4.5 & 54.5 $\pm$ 0.6 & 45.7 $\pm$ 2.2\\ 
 & F1  & 65.8 $\pm$ 4.5 & 54.5 $\pm$ 0.6 & 45.7 $\pm$ 2.2\\
 & PP  & 3.310 $\pm$ 0.478 & 2.895 $\pm$ 0.014 & 2.990 $\pm$ 0.010\\ \hline
\multirow{3}{*}{\texttt{Unigram}}
 & acc & 47.3 & 47.2 & 48.3 \\ 
 & F1  & 47.3  & 47.2 & 48.3\\
 & PP  & 3.104 & 3.113 & 3.038\\ \hline
\end{tabular}
\caption{Performance of our models for 1, 5 and 30 masked atoms per molecule. acc is octet rule accuracy, $F1$ is octet rule F1-micro score and PP is the sample perplexity, each are averaged over the test set. The uncertainty corresponds to the standard deviation of ten models, trained with different start seed.}
\label{tab:n_mask}
\end{table}






\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/qm9/octet_f1_micro_vs_nmask_qm9.pdf}
    \caption{}
    \label{fig:f1_micro_vs_nmask_qm9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/qm9/octet_f1_macro_vs_nmask_qm9.pdf}
    \caption{}
    \label{fig:f1_macro_vs_nmask_qm9}
    \end{subfigure}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/qm9/octet_f1_micro_vs_length_qm9.pdf}
    \caption{}
    \label{fig:f1_micro_vs_length_qm9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/qm9/octet_f1_mAcro_vs_length_qm9.pdf}
    \caption{}
    \label{fig:f1_macro_vs_length_qm9}
    \end{subfigure}
    \caption{Octet F1 micro (a) and octet F1 macro (b) evaluated by different number of masked atoms. Octet F1 micro (c) and Octet F1 macro (d) evaluated on molecules of varying size, with 1 atom masked. Error bar corresponds to standard deviation of 10 models trained with different start seed}
    \label{fig:qm9_extended_results}
\end{figure}





The transformer model is very flexible in terms of modeling capability, like any other deep learning model, so to gauge complexity of the task, we evaluate five \texttt{binary-transformer} models of various sizes, which can be seen in Table \ref{tab:transformer_models}.
Here we see that even very small transformer models perform well. As the models increase in number of parameters the performance increases, which however comes at a cost of computation and memory consumption.\\

\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Model & Metric & $n_{mask}=1$ & $n_{mask}=5$ & $t_{train}$ (min) & Parameters\\ \hline

\multirow{3}{*}{\texttt{layers=1, heads=1, $d_{emb}$=4}}
 & acc & 86.0 & 85.8 & \multirow{3}{*}{60} & \multirow{3}{*}{199} \\ 
 & F1  & 86.0  & 85.8 &&\\
 & PP  & 1.426 & 1.441 && \\ \hline
 
 \multirow{3}{*}{\texttt{layers=2, heads=1, $d_{emb}$=4}}
 & acc & 89.9 & 89.8 & \multirow{3}{*}{63} & \multirow{3}{*}{265} \\ 
 & F1  & 89.9  & 89.8 && \\
 & PP  & 1.261 & 1.272 && \\ \hline
 
\multirow{3}{*}{\texttt{layers=2, heads=3, $d_{emb}$=64}}
 & acc & 96.3 & 94.4 & \multirow{3}{*}{77} & \multirow{3}{*}{118149} \\ 
 & F1  & 96.3  & 94.4 &&\\
 & PP  & 1.089 & 1.130 && \\ \hline
 
\multirow{3}{*}{\texttt{layers=4, heads=3, $d_{emb}$=64}}
 & acc & 98.4 & 97.4 & \multirow{3}{*}{82} & \multirow{3}{*}{234885} \\ 
 & F1  & 98.4  & 97.3 && \\
 & PP  & 1.031 & 1.056 && \\ \hline
 
 \multirow{3}{*}{\texttt{layers=8, heads=6, $d_{emb}$=64}}
 & acc & 99.8 & 97.9 & \multirow{3}{*}{110} & \multirow{3}{*}{866181} \\ 
 & F1  & 99.8  & 97.9 && \\
 & PP  & 1.008 & 1.045 && \\ \hline

\end{tabular}
\caption{Performance of \texttt{binary-transformer} models with different number of trainable parameters, for 1 and 5 masked atoms per molecule. $acc$ is octet accuracy, $F1$ is octet F1-score and $PP$ is perplexity, each averaged over the test set. $t_{train}$ is the training time.}
\label{tab:transformer_models}
\end{table}

 \section{Zinc extended results}
\label{sec:appendix:zinc}


From Figure \ref{fig:confusion_zinc_1} we see that both our transformer models, has learn to discriminate between certain elements, that under the octet-rule should be indistinguishable, like F, but also to allow for ions, in the form of $\text{O}^-$.

A similar story can be seen in Figure \ref{fig:confusion_zinc_2}, where we have ambiguity between O,S but also $\text{N}^-$ ions.

Figure \ref{fig:confusion_zinc_3},\ref{fig:confusion_zinc_5} and \ref{fig:confusion_zinc_6} does not provide any insights, as the dataset is too bias, and almost only contain one type of element for each number of covalent bonds.  


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/confusion1.pdf}
    \caption{Confusion matrix for cases where the masked atom has one covalent bond.}
    \label{fig:confusion_zinc_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/confusion2.pdf}
    \caption{Confusion matrix for cases where the masked atom has two covalent bond.}
    \label{fig:confusion_zinc_2}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/confusion3.pdf}
    \caption{Confusion matrix for cases where the masked atom has three covalent bond.}
    \label{fig:confusion_zinc_3}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/confusion5.pdf}
    \caption{Confusion matrix for cases where the masked atom has five covalent bond.}
    \label{fig:confusion_zinc_5}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/confusion6.pdf}
    \caption{Confusion matrix for cases where the masked atom has six covalent bond.}
    \label{fig:confusion_zinc_6}
\end{figure}

Figure \ref{fig:perplexity_vs_masks_zinc},\ref{fig:metrics_vs_nmask_zinc} we see the save story as underlined in the main text, namely that the \texttt{Bond-transformer} outperforms and the \texttt{Binary-transformer} also performs similar or better than the \texttt{octet-rule-unigram} model, depending on the number of masks, and metric used to evaluate.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/sample_perplexity_vs_nmask_zinc.pdf}
    \caption{Sample perplexity evaluated by different number of masked atoms. Error bar corresponds to standard deviation of 10 models trained with different start seed}
    \label{fig:perplexity_vs_masks_zinc}
\end{figure}



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/octet_f1_micro_vs_nmask_zinc.pdf}
    \caption{}
    \label{fig:octet_f1_micro_vs_nmask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/octet_f1_macro_vs_nmask_zinc.pdf}
    \caption{}
    \label{fig:octet_f1_macro_vs_nmask}
    \end{subfigure}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/sample_f1_micro_vs_nmask_zinc.pdf}
    \caption{}
    \label{fig:f1_micro_vs_nmask}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/sample_f1_macro_vs_nmask_zinc.pdf}
    \caption{}
    \label{fig:f1_macro_vs_nmask}
    \end{subfigure}
    
    \caption{Octet F1 micro (a), octet F1 macro (b), sample F1 micro (c) and sample F1 macro (d) evaluated by different number of masked atoms. Error bar corresponds to standard deviation of 10 models trained with different start seed}
    \label{fig:metrics_vs_nmask_zinc}
\end{figure}



\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/octet_f1_micro_vs_length.pdf}
    \caption{}
    \label{fig:octet_f1_micro_vs_length}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/octet_f1_macro_vs_length.pdf}
    \caption{}
    \label{fig:octet_f1_macro_vs_length}
    \end{subfigure}
     \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/sample_f1_micro_vs_length.pdf}
    \caption{}
    \label{fig:f1_micro_vs_length}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SI/images/zinc/sample_f1_macro_vs_length.pdf}
    \caption{}
    \label{fig:f1_macro_vs_length}
    \end{subfigure}
    \caption{Octet F1 micro (a), octet F1 macro (b), sample F1 micro (c) and sample F1 macro (d) evaluated on molecules of varying size, with 1 atom masked. Error bar corresponds to standard deviation of 10 models trained with different start seed}
\end{figure}
 \end{document}
 \end{document}
